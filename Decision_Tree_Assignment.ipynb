{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Decision Tree - Assignment Questions & Answers :**"
      ],
      "metadata": {
        "id": "Rhar8QWN2gSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vZKxr0UuGuo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.1. What is a Decision Tree, and how does it work in the context of classification?**\n",
        "  \n",
        "  ans1- A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.  It works by creating a model that resembles an upside-down flowchart or tree, where each internal node represents a test on a specific feature, each branch represents the outcome of that test, and each leaf node represents a class label (in classification) or a predicted value (in regression). The goal is to predict the value of a target variable by learning simple decision rules from the data's features.\n",
        "  \n",
        "  **How It Works for Classification**\n",
        "  \n",
        "   The decision tree algorithm uses a \"divide and conquer\" strategy to recursively partition the dataset into smaller and smaller subsets. The process for classification typically involves these key steps:\n",
        "  \n",
        "  1. Start with the Root Node: The algorithm begins with the entire dataset at the root of the tree.\n",
        "\n",
        "  2. Select the Best Split: At each internal node, the algorithm evaluates all available features to find the one that best divides the data into the most homogeneous subsets. This is determined using a metric like Gini impurity or information gain. The chosen feature for the split is the one that results in the greatest reduction in impurity or the highest information gain.\n",
        "\n",
        "  3. Split the Data: The node is then split based on the values of the selected feature, creating new branches and child nodes. For example, a \"weather\" feature might be split into \"sunny,\" \"overcast,\" and \"rainy\" branches.\n",
        "\n",
        "  4. Repeat Recursively: The process is repeated for each new child node, using only the data that falls into that branch. This continues until a stopping criterion is met, such as:\n",
        "\n",
        "   * All data points in a node belong to the same class (a \"pure\" node).\n",
        "   * The tree reaches a predefined maximum depth.\n",
        "   * The number of data points in a node is below a minimum threshold.\n",
        "\n",
        "  5. Assign Leaf Nodes: Once the process stops, the terminal nodes are assigned a class label. For a new, unseen data point, the algorithm traverses the tree from the root, following the branches that match its feature values until it reaches a leaf node. The class label of that leaf node is the model's prediction for that data point.\n",
        "\n",
        "\n",
        "\n",
        "**Q.2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "\n",
        " ans2 - Gini Impurity and Entropy are two common measures of impurity used in Decision Tree algorithms to determine the best way to split a node. A \"pure\" node is one where all the data points belong to the same class. The goal of a decision tree is to create splits that lead to the most pure nodes possible. Both Gini Impurity and Entropy quantify this \"impurity\" or \"disorder.\"\n",
        "\n",
        "\n",
        "  **Gini Impurity**\n",
        "  \n",
        "  Gini Impurity measures the probability that a randomly chosen data point in a node would be incorrectly classified if it were randomly labeled according to the distribution of classes in that node. Its value ranges from 0 to 0.5. A Gini Impurity of 0 means the node is perfectly pure (all data points belong to a single class), while a value of 0.5 indicates maximum impurity, where the classes are evenly distributed.\n",
        "\n",
        "  The formula for Gini Impurity for a given node is:\n",
        "\n",
        "  \n",
        "  Gini = 1 - \\sum_{i=1}^{c}p_{i}^{2}\n",
        "\n",
        "  where p_i is the probability of an element being classified as class i and c is the number of classes.\n",
        "\n",
        "\n",
        "  **Entropy**\n",
        "  \n",
        "  Entropy is a concept from information theory that measures the amount of uncertainty or randomness in a dataset. In the context of decision trees, it quantifies the impurity of a node by measuring the uncertainty in the distribution of class labels. Its value ranges from 0 to 1 for binary classification. An entropy of 0 means the node is perfectly pure, while an entropy of 1 indicates maximum disorder (equal class distribution).\n",
        "\n",
        "  \n",
        "  The formula for Entropy for a given node is:\n",
        "\n",
        "\n",
        "  Entropy = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
        "\n",
        "\n",
        "  where p_i is the probability of an element being classified as class i and c is the number of classes.\n",
        "\n",
        "\n",
        "  **How They Impact Decision Tree Splits**\n",
        "\n",
        "  \n",
        "  Decision trees use these measures to decide which feature and what split point to use at each node. The algorithm's objective is to find the split that results in the greatest reduction in impurity or, in other words, the highest Information Gain. Information Gain is simply the difference between the parent node's impurity and the weighted average of the children nodes' impurities after the split.\n",
        "\n",
        "    * Impact on Splits: Both measures guide the tree-building process by favoring splits that create more homogeneous child nodes.\n",
        "\n",
        "    * Computational Difference: Gini Impurity is generally faster to compute because it does not involve the more complex logarithmic calculations required for Entropy.\n",
        "\n",
        "    * Split Behavior: While both measures often lead to similar splits, there can be subtle differences. Gini Impurity tends to isolate the most frequent class in a node, whereas Entropy can sometimes produce slightly more balanced splits. However, in practice, the results from using either metric are often very similar.\n",
        "\n",
        "\n",
        "\n",
        "**Q.3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "\n",
        " ans3 - Pruning is a technique used in decision trees to reduce the complexity of the model and prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on new, unseen data. Pre-pruning and post-pruning are two distinct approaches to achieving this.\n",
        "\n",
        "  \n",
        "  **re-Pruning (Early Stopping)**\n",
        "\n",
        "  \n",
        "  Pre-pruning involves stopping the growth of the decision tree before it is fully grown. This is done by setting certain stopping criteria during the tree's construction phase. Common criteria include:\n",
        "\n",
        "  \n",
        "  * Maximum depth of the tree: Limiting how many levels the tree can have.\n",
        "\n",
        "  * Minimum number of samples per leaf node: Ensuring each leaf node contains a minimum number of data points.\n",
        "\n",
        "  * Minimum decrease in impurity: Stopping the split if the information gain (or Gini impurity reduction) is below a certain threshold.\n",
        "\n",
        "\n",
        "  **Practical Advantage:**   The main advantage of pre-pruning is efficiency. Since you stop the tree from growing early, the training process is significantly faster and requires less memory. This is particularly useful for very large datasets where building a full tree would be computationally expensive.\n",
        "\n",
        "\n",
        "  **Post-Pruning**\n",
        "  \n",
        "  Post-pruning involves building a full decision tree first, and then trimming back the branches or nodes that do not add significant predictive power. The process typically works by evaluating the performance of the tree on a separate validation dataset. A common method is to replace a subtree with a single leaf node if the performance on the validation set does not decrease (or improves) after the replacement.\n",
        "\n",
        "\n",
        "  **Practical Advantage:**\n",
        "   A key advantage of post-pruning is that it can lead to a more optimal and accurate model. By allowing the tree to grow fully, it has the opportunity to explore all possible splits, including those that might appear insignificant at a higher level but could lead to a significant gain deeper in the tree. This helps avoid the \"horizon effect,\" where pre-pruning might stop a split too early and miss a valuable subsequent split.\n",
        "\n",
        "\n",
        "**Q.4.What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "\n",
        " ans4 - Information Gain (IG) is a metric used in decision tree algorithms to determine the effectiveness of a feature in splitting a dataset. It quantifies how much a particular feature reduces the entropy (or impurity/uncertainty) of the data. Essentially, it measures the \"information\" gained by knowing the value of a feature.\n",
        "\n",
        "  \n",
        "  **How it is Calculated**\n",
        "\n",
        "  \n",
        "  Information Gain is calculated using the following formula:\n",
        "\n",
        "  \n",
        "  IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\n",
        "  \n",
        "  where:\n",
        "\n",
        "    * IG(S, A) is the information gain of feature A on a dataset S.\n",
        "\n",
        "    * Entropy(S) is the entropy of the parent node (the dataset before the split).\n",
        "\n",
        "    * Values(A) is the set of all possible values for feature A.\n",
        "\n",
        "    * S_v is the subset of data where feature A has the value v.\n",
        "    \n",
        "    * |S_v| is the number of data points in the subset S_v.0\n",
        "\n",
        "    * |S| is the total number of data points in the parent dataset.\n",
        "\n",
        "  * The term \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v) is the weighted average entropy of the child nodes created after the split.\n",
        "\n",
        "\n",
        "  **Why It's Important for Choosing the Best Split**\n",
        "\n",
        "\n",
        "  \n",
        "  Information Gain is crucial because it provides a quantitative way to select the best feature for splitting a node. A decision tree algorithm, at each step, evaluates all potential features and their possible split points. The goal is to find the split that leads to the most homogeneous (or \"pure\") child nodes.\n",
        "  \n",
        "  The feature with the highest information gain is chosen for the split. A high information gain means the split significantly reduces the uncertainty of the dataset, leading to child nodes where the data points are more uniform in their class distribution. By repeatedly selecting the feature with the highest information gain, the decision tree efficiently reduces the total entropy of the system and builds a path that effectively classifies data points.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Q.5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "\n",
        "  ans5- Decision trees are versatile supervised learning algorithms used for both classification and regression tasks. They model decisions and their possible outcomes in a tree-like structure, making them intuitive and easy to interpret.\n",
        "\n",
        "  \n",
        "  **Common Real-World Applications**\n",
        "\n",
        "  \n",
        "  * Medical Diagnosis : Decision trees can help doctors diagnose diseases by analyzing patient symptoms, test results, and medical history. The tree can guide a doctor through a series of questions to narrow down a diagnosis.\n",
        "\n",
        "\n",
        "  * Credit Risk Assessment : Financial institutions use decision trees to determine if a loan applicant is likely to default. The model analyzes factors like income, credit score, and debt-to-income ratio to make a \"yes\" or \"no\" decision on loan approval.\n",
        "\n",
        "  \n",
        "  * Customer Relationship Management (CRM) 📈: Companies use decision trees to predict customer behavior, such as whether a customer will churn (cancel their service) or respond positively to a marketing campaign. This helps them tailor strategies for customer retention and targeted marketing.\n",
        "\n",
        "\n",
        "  * E-commerce & Product Recommendation 🛒: E-commerce sites can use decision trees to recommend products to customers based on their browsing history, past purchases, and other demographic information.\n",
        "\n",
        "  **Main Advantages**\n",
        "\n",
        "\n",
        "  * Interpretability: They are easy to understand and visualize, even for non-technical people. The logic behind the model's decisions is transparent, which is often called a \"white-box\" model.\n",
        "\n",
        "\n",
        "  * Little Data Preparation: They require less data preprocessing than many other algorithms. They can handle both numerical and categorical data, and they don't require data scaling or normalization.\n",
        "\n",
        "  * Handles Non-linear Relationships: Decision trees can capture complex, non-linear relationships between features and the target variable effectively.\n",
        "\n",
        "  \n",
        "  * Versatility: They can be used for both classification (predicting a category, like \"spam\" or \"not spam\") and regression (predicting a continuous value, like house prices).\n",
        "\n",
        "\n",
        "  **Main Limitations**\n",
        "\n",
        "\n",
        "  * Overfitting: This is their most significant limitation. A fully-grown decision tree can become too complex and perfectly fit the training data, including its noise, leading to poor performance on new data. This is why pruning is a crucial step.\n",
        "\n",
        "  \n",
        "  * Instability: Small changes in the training data can result in a completely different tree structure. This lack of stability can make the model difficult to reproduce and less reliable.\n",
        "\n",
        "\n",
        "  * Bias with Imbalanced Data: When the dataset has a class imbalance (e.g., a small number of fraud cases compared to non-fraud cases), decision trees can be biased towards the majority class and may not perform well on the minority class.\n",
        "  \n",
        "\n",
        "  * Greedy Approach: The algorithm uses a \"greedy\" approach at each node by selecting the best split locally, which doesn't guarantee the globally optimal tree structure."
      ],
      "metadata": {
        "id": "AnMjr0kR4HG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.6. Dataset Info:**\n",
        "\n",
        "**● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).**\n",
        "\n",
        "**● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).**\n",
        "\n",
        "**Write a Python program to:**\n",
        "\n",
        "**● Load the Iris Dataset**\n",
        "\n",
        "**● Train a Decision Tree Classifier using the Gini criterion**\n",
        "\n",
        "**● Print the model's accuracy and feature importances**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "fUlViB08oIQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# The criterion is 'gini' by default, but we specify it for clarity\n",
        "dtree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dtree.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy (Gini criterion): {:.2f}\".format(accuracy))\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for importance, name in sorted(zip(dtree.feature_importances_, feature_names), reverse=True):\n",
        "    print(f\"  {name}: {importance:.4f}\")\n",
        "\n",
        "# Optional: Visualize the tree for better understanding\n",
        "# from six import StringIO\n",
        "# from IPython.display import Image\n",
        "# import pydotplus\n",
        "\n",
        "# dot_data = StringIO()\n",
        "# export_graphviz(dtree, out_file=dot_data,\n",
        "#                 filled=True, rounded=True,\n",
        "#                 special_characters=True,\n",
        "#                 feature_names=feature_names,\n",
        "#                 class_names=target_names)\n",
        "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "# Image(graph.create_png())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXDGezp2oG97",
        "outputId": "097de39c-d798-4358-e782-4a39f4e4d7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (Gini criterion): 1.00\n",
            "------------------------------\n",
            "Feature Importances:\n",
            "  petal length (cm): 0.9061\n",
            "  petal width (cm): 0.0772\n",
            "  sepal width (cm): 0.0167\n",
            "  sepal length (cm): 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.7.  Write a Python program to:**\n",
        "\n",
        "**● Load the Iris Dataset**\n",
        "\n",
        "**● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "61kF3YMupIVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with a maximum depth of 3 (pre-pruning)\n",
        "dtree_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dtree_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (no depth limit)\n",
        "dtree_full = DecisionTreeClassifier(random_state=42)\n",
        "dtree_full.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy for the pruned tree\n",
        "y_pred_pruned = dtree_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "print(f\"Accuracy of the pruned tree (max_depth=3): {accuracy_pruned:.2f}\")\n",
        "\n",
        "# Predict and calculate accuracy for the fully-grown tree\n",
        "y_pred_full = dtree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of the fully-grown tree: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDFxVw3_oXCq",
        "outputId": "d63d3452-bf32-41c2-9be2-ece11d34ee4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the pruned tree (max_depth=3): 1.00\n",
            "Accuracy of the fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.8. Write a Python program to:**\n",
        "\n",
        "**● Load the California Housing dataset from sklearn**\n",
        "\n",
        "**● Train a Decision Tree Regressor**\n",
        "\n",
        "**● Print the Mean Squared Error (MSE) and feature importances**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "GuFNzQrVqPvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "# We're using the default parameters here\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for importance, name in sorted(zip(regressor.feature_importances_, feature_names), reverse=True):\n",
        "    print(f\"  {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grn2vMOkqLgR",
        "outputId": "0772d157-c423-4ea6-8a83-59366fbfbc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "------------------------------\n",
            "Feature Importances:\n",
            "  MedInc: 0.5285\n",
            "  AveOccup: 0.1308\n",
            "  Latitude: 0.0937\n",
            "  Longitude: 0.0829\n",
            "  AveRooms: 0.0530\n",
            "  HouseAge: 0.0519\n",
            "  Population: 0.0305\n",
            "  AveBedrms: 0.0287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.9. Write a Python program to:**\n",
        "\n",
        "**● Load the Iris Dataset**\n",
        "\n",
        "**● Tune the Decision Tree's max_depth and min_samples_split using GridSearchCV**\n",
        "\n",
        "**● Print the best parameters and the resulting model accuracy**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "IpGhsa2Dq9au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "# Instantiate the Decision Tree Classifier\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Instantiate GridSearchCV with n_jobs=1 to avoid parallel processing issues\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=1)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Get the best model and make predictions on the test set\n",
        "best_dtree = grid_search.best_estimator_\n",
        "y_pred = best_dtree.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with best parameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIjce9qdq3Cd",
        "outputId": "40909de9-258a-404c-d12c-c4cebdf920e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "------------------------------\n",
            "Accuracy with best parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.10. Imagine you're working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "\n",
        "**Explain the step-by-step process you would follow to:**\n",
        "\n",
        "**● Handle the missing values**\n",
        "\n",
        "**● Encode the categorical features**\n",
        "\n",
        "**● Train a Decision Tree model**\n",
        "\n",
        "**● Tune its hyperparameters**\n",
        "\n",
        "**● Evaluate its performance**\n",
        "\n",
        "**And describe what business value this model could provide in the real-world setting.**\n",
        "\n",
        "  ans10- I would follow a structured process to build, evaluate, and deploy a Decision Tree model to predict a patient's disease, ensuring the model is robust and provides clear business value.\n",
        "  \n",
        "  **Step-by-Step Process**\n",
        "\n",
        "  1. Handle Missing Values\n",
        "\n",
        "  First, I'd analyze the missing data to understand the extent and nature of the missingness. For a large dataset, I would:\n",
        "  \n",
        "  * For Numerical Features: Use a method like mean, median, or mode imputation to fill in the missing values. The choice depends on the data's distribution. Median imputation is robust to outliers, making it a good default choice.\n",
        "  * For Categorical Features: Use a new category like \"Unknown\" or the mode (most frequent category) to impute missing values.\n",
        "  * Create Indicator Variables: For a more advanced approach, I could create a new binary column for each feature that had missing values, indicating whether the original value was missing or not. This lets the model learn if the missingness itself is a meaningful pattern.\n",
        "\n",
        "  2. Encode Categorical Features\n",
        "  \n",
        "  Decision Trees require numerical input, so categorical features must be converted. I would use:\n",
        "\n",
        "  * One-Hot Encoding: This is the most common method for nominal (unordered) categorical variables. It creates a new binary column for each category, which is ideal for Decision Trees as it avoids a false sense of ordinality.\n",
        "  * Label Encoding: I would only use this for ordinal (ordered) categorical variables, where the categories have a natural ranking (e.g., 'low', 'medium', 'high').\n",
        "\n",
        "  3. Train a Decision Tree Model\n",
        "  \n",
        "  Once the data is preprocessed, I would train the Decision Tree.\n",
        "  \n",
        "  * Data Split: I'd split the dataset into three parts: a training set (e.g., 70% of the data) for model training, a validation set (e.g., 15%) for hyperparameter tuning, and a test set (e.g., 15%) for final, unbiased performance evaluation.\n",
        "  * Model Instantiation: I would instantiate a DecisionTreeClassifier from a library like Scikit-learn, using a criterion like Gini Impurity or Entropy to measure the quality of splits.\n",
        "\n",
        "  4. Tune Hyperparameters\n",
        "\n",
        "  To prevent overfitting and find the optimal model, I would tune its hyperparameters.\n",
        "  \n",
        "  * Define Parameter Grid: I would create a dictionary of hyperparameters to search, such as max_depth (to control model complexity), min_samples_leaf (to prevent overfitting to small groups), and min_samples_split.\n",
        "  * Use GridSearchCV or RandomizedSearchCV: I'd use GridSearchCV or RandomizedSearchCV with cross-validation on the training set to systematically test different combinations of these hyperparameters. This process automatically finds the best combination that yields the highest accuracy on the validation folds.\n",
        "\n",
        "  5. Evaluate Performance\n",
        "  \n",
        "  The final, tuned model's performance would be evaluated on the unseen test set.\n",
        "  \n",
        "  * Key Metrics: For a classification problem like disease prediction, I would focus on metrics beyond just accuracy, such as:\n",
        "  * Precision: Of all the patients predicted to have the disease, how many actually have it?\n",
        "  * Recall (Sensitivity): Of all the patients who actually have the disease, how many did the model correctly identify? This is often a critical metric in healthcare to avoid false negatives.\n",
        "  * F1-Score: The harmonic mean of precision and recall, providing a balanced measure.\n",
        "  * ROC Curve and AUC: To assess the model's ability to discriminate between the two classes.\n",
        "  \n",
        "  **Real-World Business Value**\n",
        "\n",
        "  This predictive model can provide significant business value by improving patient outcomes and optimizing resource allocation.\n",
        "  \n",
        "  * Early Diagnosis: The model can identify patients at high risk of a disease based on their unique combination of features. This allows healthcare providers to intervene early, leading to better treatment outcomes and potentially saving lives.\n",
        "  * Resource Optimization: By identifying high-risk patients, hospitals and clinics can allocate resources more effectively. For example, they can prioritize follow-up appointments, diagnostic tests, or specialized care for those most likely to be sick, reducing unnecessary costs and wait times for low-risk individuals.\n",
        "  * Improved Clinical Decision Support: The Decision Tree's transparent, rule-based nature makes it an excellent tool for clinical decision support. Doctors can trace the model's decision path (e.g., \"If patient A's blood pressure is > X, and cholesterol is > Y, the model predicts the disease\"), which builds trust and helps them validate the recommendations."
      ],
      "metadata": {
        "id": "ZPstRPZksE-V"
      }
    }
  ]
}