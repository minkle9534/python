{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression - Assignment Questions & Answers**"
      ],
      "metadata": {
        "id": "3CAW5ROAnYdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "  - Logistic Regression is a statistical and machine learning model used to predict a categorical or binary outcome. Instead of predicting a continuous value like linear regression, it predicts the probability that an observation belongs to a particular category. It does this by using a sigmoid function to map the output of a linear equation to a value between 0 and 1, which represents a probability. For example, it can be used to predict whether a customer will churn (yes or no), or if an email is spam (spam or not spam).\n",
        "\n",
        "  Key Differences between Logistic and Linear Regression\n",
        "  \n",
        "  Feature | Linear Regression | Logistic Regression |\n",
        "  |---|---|---||\n",
        "  \n",
        "  Type of Problem | Solves regression problems (predicting a continuous value). | Solves classification problems (predicting a categorical outcome). ||\n",
        "  \n",
        "  Output | Predicts a continuous value (e.g., house price, temperature). | Predicts a probability between 0 and 1. ||\n",
        "  \n",
        "  Output Type | The dependent variable is continuous and real-valued. | The dependent variable is categorical or binary (e.g., 0/1, yes/no). ||\n",
        "\n",
        "  Underlying Function | Uses a linear equation (y = mx + b) to fit a straight line to the data. | Uses the sigmoid function (or logistic function) to transform the linear equation's output into a probability. ||\n",
        "  \n",
        "  Curve Shape | The relationship between the independent and dependent variables is a straight line. | The relationship is represented by an S-shaped curve.|\n",
        "\n",
        "\n",
        "**Q.2. Explain the role of the Sigmoid function in Logistic Regression.**\n",
        "  - The Sigmoid function, also known as the logistic function, is the core component that enables logistic regression to perform classification. Its primary role is to transform the output of a linear equation into a probability score that falls between 0 and 1.\n",
        "\n",
        "  Here's a breakdown of its role:\n",
        "  1. Mapping to Probability: A linear regression model's output can be any real number, from negative infinity to positive infinity. This is not suitable for classification problems, where the output should represent a probability. The sigmoid function takes this linear output and \"squashes\" it into an S-shaped curve.\n",
        "    * As the linear output approaches positive infinity, the sigmoid function's output gets closer to 1.\n",
        "    * As the linear output approaches negative infinity, the sigmoid function's output gets closer to 0.\n",
        "  \n",
        "  2. Binary Classification: In logistic regression, the output of the sigmoid function is interpreted as the probability of the observation belonging to the positive class (e.g., class \"1\"). A decision boundary (usually 0.5) is then applied to this probability to classify the observation. If the probability is greater than or equal to the threshold, it's classified as the positive class; otherwise, it's the negative class.\n",
        "  \n",
        "  This process allows logistic regression to model the probability of a binary outcome, effectively bridging the gap between a linear model and a classification problem.\n",
        "\n",
        "\n",
        "**Q.3. What is Regularization in Logistic Regression and why is it needed?**\n",
        "  - Regularization in Logistic Regression is a technique used to prevent overfitting, which occurs when a model performs exceptionally well on the training data but poorly on new, unseen data. It achieves this by adding a penalty term to the model's loss function, which discourages the model from assigning excessively large weights (coefficients) to features.\n",
        "\n",
        "  Why is Regularization Needed?\n",
        "\n",
        "  Regularization is crucial in logistic regression for a few key reasons:\n",
        "  1. Prevents Overfitting: In a complex dataset with many features, a logistic regression model can \"memorize\" the training data, including its noise and random fluctuations. This results in an overly complex model with high-magnitude coefficients, which is a sign of overfitting. Regularization directly addresses this by penalizing those large coefficients, forcing the model to be simpler and more general.\n",
        "  \n",
        "  2. Handles Multicollinearity: When independent variables are highly correlated (a phenomenon called multicollinearity), the model's coefficients can become unstable and difficult to interpret. Regularization helps by either shrinking these coefficients (L2 regularization) or setting some of them to zero (L1 regularization), which improves the model's stability.\n",
        "  \n",
        "  3. Improves Model Generalization: By keeping the model's complexity in check, regularization ensures that the model learns the underlying patterns in the data rather than the noise. This leads to a better-performing model that can generalize more effectively to new, unseen data.\n",
        "  \n",
        "  The two most common types of regularization used in logistic regression are L1 (Lasso) and L2 (Ridge) regularization, each adding a different type of penalty to the loss function.\n",
        "\n",
        "\n",
        "**Q.4. What are some common evaluation metrics for classification models, and why are they important?**\n",
        "  - Common evaluation metrics for classification models are used to quantitatively assess a model's performance. They're crucial because they provide a deeper, more nuanced understanding of how a model is performing, especially in scenarios where a simple accuracy score can be misleading.\n",
        "\n",
        "  Key Metrics and Their Importance\n",
        "  1. Accuracy: The most intuitive metric. It's the ratio of correctly predicted observations to the total observations.\n",
        "   * Importance: It gives a quick, easy-to-understand measure of overall model performance. However, it can be a deceptive metric, especially with imbalanced datasets, where one class is much more frequent than others. For example, a model that simply predicts \"no disease\" for every patient might achieve 99% accuracy if only 1% of the patients are actually sick. This model is useless but appears highly accurate.\n",
        "  \n",
        "  2. Confusion Matrix: A table that summarizes the performance of a classification model. It shows the number of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "   * Importance: This is the foundation for most other metrics. It helps to visualize the types of errors a model is making, providing a much clearer picture than a single accuracy score.\n",
        "\n",
        "  3. Precision: The ratio of correctly predicted positive observations to the total predicted positive observations. It answers the question, \"Of all the times the model predicted 'yes,' how many were actually 'yes'?\"\n",
        "   * Importance: It's crucial when the cost of a False Positive is high. For example, in a spam filter, high precision is desirable to ensure that important, non-spam emails aren't incorrectly flagged as spam.\n",
        "\n",
        "  4. Recall (or Sensitivity): The ratio of correctly predicted positive observations to all observations in the actual class. It answers the question, \"Of all the actual 'yes' cases, how many did the model correctly identify?\"\n",
        "   * Importance: This metric is vital when the cost of a False Negative is high. For example, in a medical diagnosis model for a serious disease, high recall is critical to ensure that as many sick patients as possible are correctly identified, even if it means some healthy patients are flagged for further testing.\n",
        "\n",
        "  5. F1-Score: The harmonic mean of precision and recall.\n",
        "   * Importance: It provides a single score that balances both precision and recall. The F1-score is particularly useful when you need to find a balance between minimizing false positives and false negatives, especially with imbalanced datasets.\n",
        "\n",
        "  6. ROC Curve and AUC: The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at various classification thresholds. The Area Under the Curve (AUC) measures the entire area underneath the ROC curve.\n",
        "   * Importance: AUC provides a single value that represents the model's ability to distinguish between classes. A score of 1.0 is a perfect model, while 0.5 is a random guess. The ROC curve and AUC are valuable because they evaluate a model's performance across all possible classification thresholds, making them robust against imbalanced data."
      ],
      "metadata": {
        "id": "KEOCkvGyn2uS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.5. Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "**(Include yoPython program that loads the breast cancer dataset from the sklearn library, ur Python code and output in the code box below.)**\n",
        "  -Python Program for Logistic Regression\n",
        "Here is a trains a Logistic Regression model, and evaluates its accuracy. The code includes comments to explain each step of the process."
      ],
      "metadata": {
        "id": "caP9504in2q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 2. Split data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Initialize and train the Logistic Regression model\n",
        "# We set max_iter to a higher value to ensure convergence\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62BayZ4duj6W",
        "outputId": "59cef329-2e1f-49a4-bdc3-437f28f8c5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.6. Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - Python Program with L2 Regularization\n",
        "  Here's a Python program that trains a Logistic Regression model with L2 regularization (also known as Ridge regularization). The LogisticRegression class in scikit-learn uses L2 regularization by default. The regularization strength is controlled by the C parameter, where a smaller C indicates stronger regularization.\n",
        "\n",
        "  The program will load the breast cancer dataset, train a model with a specified C value, and then print the resulting coefficients and the model's accuracy."
      ],
      "metadata": {
        "id": "fxPRzgJIvnhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Logistic Regression model with L2 regularization\n",
        "# The 'penalty' parameter is 'l2' by default. 'C' is the inverse of regularization strength.\n",
        "# A smaller C value means stronger regularization. We use C=0.1 here.\n",
        "l2_model = LogisticRegression(penalty='l2', C=0.1, max_iter=10000)\n",
        "l2_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Print the model coefficients\n",
        "print(\"Model Coefficients (L2 Regularization):\")\n",
        "for feature, coef in zip(X.columns, l2_model.coef_[0]):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "\n",
        "# 5. Make predictions and print the accuracy\n",
        "y_pred = l2_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkuqZm6cw1iP",
        "outputId": "dfa9f74a-11b8-42ce-85d1-2e1790b6792c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (L2 Regularization):\n",
            "  mean radius: 0.1440\n",
            "  mean texture: 0.1311\n",
            "  mean perimeter: -0.2353\n",
            "  mean area: 0.0311\n",
            "  mean smoothness: -0.0212\n",
            "  mean compactness: -0.0410\n",
            "  mean concavity: -0.0772\n",
            "  mean concave points: -0.0379\n",
            "  mean symmetry: -0.0310\n",
            "  mean fractal dimension: -0.0063\n",
            "  radius error: -0.0112\n",
            "  texture error: 0.2199\n",
            "  perimeter error: 0.0206\n",
            "  area error: -0.0766\n",
            "  smoothness error: -0.0026\n",
            "  compactness error: 0.0013\n",
            "  concavity error: -0.0094\n",
            "  concave points error: -0.0044\n",
            "  symmetry error: -0.0045\n",
            "  fractal dimension error: 0.0007\n",
            "  worst radius: 0.0251\n",
            "  worst texture: -0.3815\n",
            "  worst perimeter: -0.1350\n",
            "  worst area: -0.0131\n",
            "  worst smoothness: -0.0423\n",
            "  worst compactness: -0.1400\n",
            "  worst concavity: -0.2183\n",
            "  worst concave points: -0.0725\n",
            "  worst symmetry: -0.0987\n",
            "  worst fractal dimension: -0.0179\n",
            "\n",
            "Model Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.7. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - Python Program for Multiclass Classification\n",
        "  Here is a Python program that demonstrates multiclass classification using a Logistic Regression model with the One-vs-Rest (ovr) strategy. The program uses the well-known Iris dataset, trains the model, and then prints a detailed classification report to evaluate its performance on each class."
      ],
      "metadata": {
        "id": "KAV8gqxfxP38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load a multiclass dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 2. Split data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Initialize and train the Logistic Regression model for multiclass classification\n",
        "# We use 'ovr' (One-vs-Rest) and 'liblinear' solver, which is efficient for this strategy.\n",
        "# Increased max_iter to ensure convergence\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Print the classification report for a detailed evaluation\n",
        "# The report includes precision, recall, f1-score, and support for each class.\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfPGIBQKyLp2",
        "outputId": "7480e9df-6de6-485b-b05e-f068a1a397fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.8. Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - Python Program for Hyperparameter Tuning with GridSearchCVHere is a Python program that uses GridSearchCV to systematically find the best C (regularization strength) and penalty (L1 or L2) hyperparameters for a Logistic Regression model. The program will load a dataset, define a grid of parameters to test, and then use cross-validation to find the optimal combination.\n",
        "\n",
        "  The results will show the best hyperparameters found and the model's accuracy on the validation set."
      ],
      "metadata": {
        "id": "PSuULPz4yTGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=10000)\n",
        "\n",
        "# 4. Define the parameter grid for GridSearchCV\n",
        "# We'll test different values for C and both L1 and L2 penalties.\n",
        "# The 'liblinear' solver supports both L1 and L2 penalties.\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# 5. Initialize GridSearchCV with the model and parameter grid\n",
        "# cv=5 means 5-fold cross-validation.\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# 6. Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Print the best parameters and best validation score\n",
        "print(\"Best hyperparameters found:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 8. Evaluate the best model on the unseen test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy of the best model: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYbN_cUIyNPl",
        "outputId": "8a56107d-70b9-4350-fd24-0170f745977b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found: {'C': 100, 'penalty': 'l1'}\n",
            "Best cross-validation accuracy: 0.9670\n",
            "Test set accuracy of the best model: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - Python Program to Demonstrate the Importance of Feature Scaling\n",
        "  Here is a Python program that trains two Logistic Regression models on the same breast cancer dataset: one without any scaling and one with StandardScaler applied to the features.\n",
        "\n",
        "  The purpose of this program is to illustrate how feature scaling can significantly impact the performance and stability of a Logistic Regression model, particularly when features have different scales."
      ],
      "metadata": {
        "id": "fNv1icFhzjLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Model 1: Without Feature Scaling ---\n",
        "\n",
        "# Initialize and train the model without scaling\n",
        "model_unscaled = LogisticRegression(max_iter=10000)\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred_unscaled = model_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Model Accuracy (without scaling): {accuracy_unscaled:.4f}\")\n",
        "\n",
        "\n",
        "# --- Model 2: With Feature Scaling ---\n",
        "\n",
        "# 3. Initialize and apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both train and test sets\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the model with scaled data\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Model Accuracy (with scaling): {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RizIAvjsza4P",
        "outputId": "90502c0a-05dd-4e6b-b114-b1860631dd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (without scaling): 0.9561\n",
            "Model Accuracy (with scaling): 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.10. Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you'd take to build a Logistic Regression model â€” including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**\n",
        "  - Given the imbalanced nature of the dataset where only 5% of customers respond, a standard logistic regression model trained on raw data would likely perform poorly. A simple model that predicts \"no response\" for every customer would achieve 95% accuracy, but would be useless for the business. Here is the approach to build a robust model for this use case.\n",
        "  \n",
        "  1. Data Handling & Preprocessing\n",
        "\n",
        "    * Feature Engineering: Convert categorical features like city, product category, or gender into numerical format using techniques like one-hot encoding. Create new features from existing ones (e.g., customer tenure, average order value) that might be predictive of a response.\n",
        "\n",
        "    * Feature Scaling: Standardize the numerical features using a StandardScaler. This is crucial for logistic regression's optimization algorithm, ensuring all features contribute equally and preventing features with larger magnitudes from dominating the model.\n",
        "\n",
        "  2. Balancing Classes\n",
        "  Since the dataset is highly imbalanced (5% vs. 95%), training the model on the raw data would bias it towards the majority class (\"no response\"). To counteract this, you should use a technique to balance the classes.\n",
        "\n",
        "    * SMOTE (Synthetic Minority Over-sampling Technique): This is the preferred method for this scenario. SMOTE creates synthetic new data points for the minority class (\"response\") by interpolating between existing minority class data points. This doesn't just duplicate data; it creates new, but plausible, samples, effectively increasing the size of the minority class.\n",
        "\n",
        "    * Alternative Methods: Other options include Undersampling, which randomly removes data from the majority class, and class_weight parameter in LogisticRegression, which gives more penalty to errors made on the minority class. However, SMOTE is generally more effective as it doesn't discard valuable data like undersampling.\n",
        "\n",
        "  3. Hyperparameter Tuning\n",
        "    * Grid Search with Cross-Validation: Use GridSearchCV to find the optimal hyperparameters. The key parameters to tune for logistic regression are:\n",
        "\n",
        "    * penalty: 'l1' (Lasso) and 'l2' (Ridge) regularization. 'l1' can be useful for feature selection as it can set some coefficients to zero.\n",
        "\n",
        "    * C: The inverse of regularization strength. A smaller C means a stronger penalty, which helps prevent overfitting.\n",
        "\n",
        "  4. Model Evaluation\n",
        "  Standard accuracy is a poor metric for this problem. You need metrics that specifically measure the model's ability to correctly identify the minority class.\n",
        "\n",
        "    * Confusion Matrix: Start by analyzing the Confusion Matrix to see the number of True Positives, False Positives, False Negatives, and True Negatives.\n",
        "\n",
        "    * Precision: This is the ratio of correctly predicted positive observations to the total predicted positive observations. It tells you, \"Of all the customers the model predicted would respond, how many actually did?\" High precision is good to avoid wasting marketing resources on uninterested customers.\n",
        "\n",
        "    * Recall: This is the ratio of correctly predicted positive observations to all observations in the actual class. It tells you, \"Of all the customers who actually responded, how many did the model correctly identify?\" High recall is crucial to ensure you don't miss out on potential customers.\n",
        "\n",
        "    * F1-Score: The harmonic mean of precision and recall. It provides a single score that balances both metrics, which is very useful when both False Positives and False Negatives are important.\n",
        "    \n",
        "    * ROC Curve and AUC: A more robust evaluation metric is the Area Under the Receiver Operating Characteristic (ROC) curve.  The AUC value summarizes the model's ability to distinguish between the two classes across all possible classification thresholds. A higher AUC (closer to 1.0) indicates a better model."
      ],
      "metadata": {
        "id": "qOI6WZAs0zPG"
      }
    }
  ]
}